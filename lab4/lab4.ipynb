{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody Obliczeniowe w Nauce i Technice\n",
    "## Laboratorium 4 - Singular Value Decomposition (Wyszukiwarka)\n",
    "### Albert Gierlach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Przygotowanie danych\n",
    "Dane przygotowano za pomocą wiki-crawlera. Wykorzystano skrypt w Pythonie (https://github.com/bornabesic/wikipedia-crawler), dostosowując go do potrzeb zadania (dodanie opcji, która pozwala pobrać N artykułów). Źródła (wikipedia.py oraz crawler.py) są dostępne w archiwum z zadaniem.\n",
    "\n",
    "Użycie:\n",
    "```\n",
    "python crawler.py N subdomain\n",
    "```\n",
    "gdzie N to liczba dokumentow do pobrania, a 'subdomain' to subdomena (użyto wartości 'en').\n",
    "Dla polepszenia rezultatów zapewniono, że długość artykułu będzie większa niż 1000 znaków.\n",
    "\n",
    "Dane w formacie .txt pobierane są do folderu ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2., 3. Określenie bag-of-words \n",
    "Stworzono klasę, która będzie przechowywać dane jednego dokumentu oraz metody pomagające w tym. Odrzucono kilka słów, które powinny zostać zignorowane podczas wyszukiwania artykułów. Całość operacji trwa około 3-5 minut. Program znajduje się w pliku lab4_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"./data\"\n",
    "cache_dir = \"./cache\"  # place for storing calculated matrices, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleData:\n",
    "    ignored_words = [\"a\", \"the\", \"of\", \"is\"]  # and probably more\n",
    "\n",
    "    def __init__(self, title):\n",
    "        self.title = title.split('.')[0]\n",
    "        self.bag_of_words = Counter()\n",
    "        self.words_vec = None\n",
    "        self.words_vec_norm = None\n",
    "\n",
    "    def load_bag_of_words(self, path):\n",
    "        with open(path, \"rt\", encoding='utf-8') as f:\n",
    "            words = re.findall(r'\\w+', f.read().lower())\n",
    "            loaded_words = [word for word in words if len(word) > 2]\n",
    "            self.bag_of_words.update(loaded_words)\n",
    "\n",
    "        for ignore_token in ArticleData.ignored_words:\n",
    "            del self.bag_of_words[ignore_token]\n",
    "\n",
    "    def create_full_bag_of_words(self, keyset, size):\n",
    "        self.words_vec = np.zeros(size)  # d_j\n",
    "        for i, k in enumerate(keyset):\n",
    "            self.words_vec[i] = self.bag_of_words[k]\n",
    "\n",
    "        self.words_vec_norm = np.linalg.norm(self.words_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag-of-words: 68581 words\n",
      "bag of words created!\n"
     ]
    }
   ],
   "source": [
    "articles_data = []\n",
    "for file in os.listdir(data_dir):\n",
    "    a_data = ArticleData(file)\n",
    "    a_data.load_bag_of_words(\"{}/{}\".format(data_dir, file))\n",
    "    articles_data.append(a_data)\n",
    "\n",
    "total_bag_of_words = Counter()\n",
    "for article in articles_data:\n",
    "    total_bag_of_words += article.bag_of_words\n",
    "\n",
    "sizeof_total = len(total_bag_of_words)\n",
    "wordset = list(total_bag_of_words.keys())\n",
    "print(\"bag-of-words: {} words\".format(sizeof_total))\n",
    "\n",
    "total_bag_of_words_vector = np.zeros(sizeof_total, dtype=int)\n",
    "\n",
    "# convert Counter to vectors\n",
    "for counter, key in enumerate(wordset):\n",
    "    total_bag_of_words_vector[counter] = total_bag_of_words[key]\n",
    "\n",
    "for article in articles_data:\n",
    "    article.create_full_bag_of_words(wordset, sizeof_total)\n",
    "    \n",
    "print(\"bag of words created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4., 5.  Rzadka macierz wektorów cech oraz IDF\n",
    "Do budowy rzadkiej macierzy wykorzystano funckję crs_matrix(). Czas operacji 3-5 minut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDF(wordset, articles_data):\n",
    "    articles_num = len(articles_data)\n",
    "    idf = []\n",
    "    for word in wordset:\n",
    "        cnt = 0\n",
    "        for article in articles_data:\n",
    "            if article.bag_of_words[word] != 0:\n",
    "                cnt += 1\n",
    "\n",
    "        idf.append(np.log10(articles_num/cnt))\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def create_sparse(articles_data, sizeof_total, idf):\n",
    "    row = []\n",
    "    column = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(articles_data)):\n",
    "        article = articles_data[i]\n",
    "        for j in range(sizeof_total):\n",
    "            if article.words_vec[j] != 0:\n",
    "                row.append(j)\n",
    "                column.append(i)\n",
    "                data.append(article.words_vec[j] * idf[j])\n",
    "\n",
    "\n",
    "    term_by_document_matirx = sparse.csr_matrix((data, (row, column)), shape=(sizeof_total, len(articles_data)))\n",
    "    return term_by_document_matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating idf\n",
      "creating sparse matrix\n",
      "term-by-document matrix: 68581x1500\n"
     ]
    }
   ],
   "source": [
    "print('calculating idf')\n",
    "idf = getIDF(wordset, articles_data)\n",
    "\n",
    "print('creating sparse matrix')\n",
    "term_by_document_matirx = create_sparse(articles_data, sizeof_total, idf)\n",
    "print(\"term-by-document matrix: {}x{}\".format(term_by_document_matirx.shape[0], term_by_document_matirx.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tak wygenerowane wyniki (macierze, wektory) zapisano do plików, aby nie musieć każdorazowo obliczać dużych macierzy na nowo, jednak na potrzeby tego notebooka zapis został pominięty.\n",
    "Wykorzystano bibliotekę dostarczaną z Pythonem - pickle. Rozmiar zapisanych plików wynosi około 800MB, gdyż głównie są to wektory, które posiadają dużo zer. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Program pozwalający na wyszukiwanie artykułów\n",
    "Stworzono nowy plik lab4_search_engine.py, który będzie odpowiedzialny za przetwarzanie danych. Na początku wczytamy zcache'owane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query(query, k, word_list, articles):\n",
    "    query = query.lower()\n",
    "    words_dict = {word: index for index, word in enumerate(word_list)}\n",
    "    words = re.findall(r'\\w+', query)\n",
    "\n",
    "    vec_query = np.zeros(len(word_list), dtype=int)\n",
    "    for w in words:\n",
    "        if w in words_dict.keys():\n",
    "            vec_query[words_dict[w]] += 1\n",
    "\n",
    "    if not np.any(vec_query):\n",
    "        print(\"No results\")\n",
    "        return\n",
    "    \n",
    "    q_norm = np.linalg.norm(vec_query)\n",
    "    res = []\n",
    "    for a in articles:\n",
    "        divider = q_norm * a.words_vec_norm\n",
    "        prod = vec_query @ a.words_vec\n",
    "        cos_theta = prod/divider\n",
    "        res.append((cos_theta, a))\n",
    "\n",
    "    res.sort(key=operator.itemgetter(0), reverse=True)\n",
    "    for res_entry in res[:k]:\n",
    "        print(res_entry[1].title)\n",
    "        \n",
    "        \n",
    "        \n",
    "# reassign variables, just for readibility\n",
    "# articles - list with all of documents (words vectors + bag of words)\n",
    "# word_list - bag_of_words_dict.keys()\n",
    "# A - sparse matrix, columns are words vectors from articles_data\n",
    "# bag_of_words - total bag_of_words vector (index 0 is occurences of the word word_list[0])\n",
    "articles, word_list, A, bag_of_words = articles_data, wordset, term_by_document_matirx, total_bag_of_words_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Text, Label, IntSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def btn(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        how_many = form.children[0].children[1].value\n",
    "        text_to_search = form.children[1].children[1].value\n",
    "        if len(text_to_search) > 1:\n",
    "            do_query(text_to_search, how_many, word_list, articles)\n",
    "        else:\n",
    "            output.append_stdout(\"Text is too short\")\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    display='flex',\n",
    "    flex_flow='row',\n",
    "    justify_content='space-between'\n",
    ")\n",
    "\n",
    "form_items = [\n",
    "    Box([Label(value='Results num'), IntSlider(min=1, max=30, value=10, descritpion='k_')], layout=form_item_layout),\n",
    "    Box([Label(value='Query'), Text(placeholder=\"Wpisz zapytanie\", descritpion='query_')], layout=form_item_layout),\n",
    "    Box([Label(), Button(description=\"Search!\")], layout=form_item_layout)\n",
    "]\n",
    "\n",
    "form = Box(form_items, layout=Layout(\n",
    "    display='flex',\n",
    "    flex_flow='column',\n",
    "    align_items='stretch',\n",
    "    width='50%'\n",
    "))\n",
    "output = Output()\n",
    "form.children[2].children[1].on_click(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef7d5273154ae59679b3c14f68ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Results num'), IntSlider(value=10, max=30, min=1)), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6691980ede7a4784936a1b4e2df0975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output) # place for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
