{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody Obliczeniowe w Nauce i Technice\n",
    "## Laboratorium 4 - Singular Value Decomposition (Wyszukiwarka)\n",
    "### Albert Gierlach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Przygotowanie danych\n",
    "Dane przygotowano za pomocą wiki-crawlera (1200 artykułów z Wikipedii). Wykorzystano skrypt w Pythonie (https://github.com/bornabesic/wikipedia-crawler), dostosowując go do potrzeb zadania (dodanie opcji, która pozwala pobrać N artykułów). Źródła (wikipedia.py oraz crawler.py) są dostępne w archiwum z zadaniem.\n",
    "\n",
    "Użycie:\n",
    "```\n",
    "python crawler.py N subdomain\n",
    "```\n",
    "gdzie N to liczba dokumentow do pobrania, a 'subdomain' to subdomena (użyto wartości 'en').\n",
    "Dla polepszenia rezultatów zapewniono, że długość artykułu będzie większa niż 200 znaków.\n",
    "\n",
    "Dane w formacie .txt pobierane są do folderu ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2., 3. Określenie bag-of-words \n",
    "Stworzono klasę, która będzie przechowywać dane jednego dokumentu oraz odpowiednie jej metody, które będą wykorzystane później. Odrzucono kilka słów, które powinny zostać zignorowane podczas wyszukiwania artykułów. Stworzono także klasę, która będzie odpowiadać za cache'owanie wyliczonych wektorów i macierzy, gdyż operacja ta trwa dość długo. Zastosowanie takiej klasy pozwala na jednokrotne wyliczenie wartości, a później wystarczy wczytać gotowe dane. Pierwsze uruchomienie trwa maksymalnie 5 minut. Wielkość cache około 500MB (jeśli zastosowanoby kompresję rozmiar zmniejszyłby się do 5MB, gdyż zawartość plików to głownie zera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rivit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rivit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Any\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "data_dir = \"./data\"\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    cache_dir = \"./cache\"  # place for storing calculated matrices, etc\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loaded = set()\n",
    "\n",
    "        if not os.path.exists(CacheManager.cache_dir):\n",
    "            os.makedirs(CacheManager.cache_dir)\n",
    "\n",
    "    def was_loaded(self, filename):\n",
    "        return filename in self.loaded\n",
    "\n",
    "    def save(self, filename, object):\n",
    "        if self.was_loaded(filename):\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open('{}/{}'.format(CacheManager.cache_dir, filename), \"wb\") as f:\n",
    "                pickle.dump(object, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"> caching \" + filename)\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    def load(self, filename):\n",
    "        try:\n",
    "            with open('{}/{}'.format(CacheManager.cache_dir, filename), \"rb\") as f:\n",
    "                res = pickle.load(f)\n",
    "                print(\"> using cached \" + filename)\n",
    "                self.loaded.add(filename)\n",
    "                return res\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class ArticleData:\n",
    "    ignored_words = [\"the\", \"for\", \"are\", \"you\"]  # and probably more\n",
    "\n",
    "    def __init__(self, title):\n",
    "        self.title = title.split('.')[0]\n",
    "        self.bag_of_words = Counter()\n",
    "        self.words_vec = None\n",
    "        self.words_vec_norm = None\n",
    "\n",
    "    def load_bag_of_words(self, path):\n",
    "        with open(path, \"rt\", encoding='utf-8') as f:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            words = re.findall(r'\\w+', f.read().lower())\n",
    "            loaded_words = [lemmatizer.lemmatize(word) for word in words if len(word) > 2 and word not in stop_words]\n",
    "            self.bag_of_words.update(loaded_words)\n",
    "\n",
    "        for ignore_token in ArticleData.ignored_words:\n",
    "            del self.bag_of_words[ignore_token]\n",
    "\n",
    "    def create_full_bag_of_words(self, keyset, size):\n",
    "        self.words_vec = np.zeros(size)  # d_j\n",
    "        for i, k in enumerate(keyset):\n",
    "            self.words_vec[i] = self.bag_of_words[k]\n",
    "\n",
    "        self.words_vec_norm = np.linalg.norm(self.words_vec)\n",
    "\n",
    "    def print_contents(self):\n",
    "        with open('{}/{}.txt'.format(data_dir, self.title), \"rt\", encoding='utf-8') as f:\n",
    "            print(f.read())\n",
    "\n",
    "    def normalize_word_vec(self):\n",
    "        self.words_vec = self.words_vec / np.linalg.norm(self.words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of articles 1218\n",
      "total number of words: 43446\n",
      "creating bag of words for every article\n",
      "created 1218 bags, every has 43446 elements\n"
     ]
    }
   ],
   "source": [
    "cache = CacheManager()\n",
    "\n",
    "articles_data: List[ArticleData] = cache.load('articles_data.dump')\n",
    "if articles_data is None:\n",
    "    articles_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        a_data = ArticleData(file)\n",
    "        a_data.load_bag_of_words(\"{}/{}\".format(data_dir, file))\n",
    "        articles_data.append(a_data)\n",
    "print(\"total number of articles {}\".format(len(articles_data)))\n",
    "\n",
    "total_bag_of_words: Counter = cache.load('total_bag_of_words.dump')\n",
    "if total_bag_of_words is None:\n",
    "    total_bag_of_words = Counter()\n",
    "    for article in articles_data:\n",
    "        total_bag_of_words += article.bag_of_words\n",
    "\n",
    "sizeof_total = len(total_bag_of_words)\n",
    "wordset: List[Any] = cache.load('wordset.dump')\n",
    "if wordset is None:\n",
    "    wordset = list(total_bag_of_words.keys())\n",
    "print(\"total number of words: {}\".format(sizeof_total))\n",
    "\n",
    "if not cache.was_loaded('articles_data.dump'):\n",
    "    print(\"creating bag of words for every article\")\n",
    "    for article in articles_data:\n",
    "        article.create_full_bag_of_words(wordset, sizeof_total)\n",
    "print(\"created {} bags, every has {} elements\".format(len(articles_data), sizeof_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4., 5.  Rzadka macierz wektorów cech oraz IDF\n",
    "Do budowy rzadkiej macierzy wykorzystano funckję crs_matrix(), która jest optymalizowana pod kątem przechowywania zer w wierszach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDF(wordset, articles_data):\n",
    "    articles_num = len(articles_data)\n",
    "    idf = []\n",
    "    for word in wordset:\n",
    "        cnt = 0\n",
    "        for article in articles_data:\n",
    "            if article.bag_of_words[word] != 0:\n",
    "                cnt += 1\n",
    "\n",
    "        idf.append(np.log10(articles_num/cnt))\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def create_sparse(articles_data, sizeof_total, idf):\n",
    "    row = []\n",
    "    column = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(articles_data)):\n",
    "        article = articles_data[i]\n",
    "        for j in range(sizeof_total):\n",
    "            if article.words_vec[j] != 0:\n",
    "                row.append(j)\n",
    "                column.append(i)\n",
    "                data.append(article.words_vec[j] * idf[j])\n",
    "\n",
    "\n",
    "    term_by_document_matirx = sparse.csr_matrix((data, (row, column)), shape=(sizeof_total, len(articles_data)))\n",
    "    return term_by_document_matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating idf\n",
      "creating sparse matrix\n",
      "term by document matrix size: 43446x1218\n"
     ]
    }
   ],
   "source": [
    "idf: List[Any] = cache.load('idf.dump')\n",
    "if idf is None:\n",
    "    print('calculating idf')\n",
    "    idf = getIDF(wordset, articles_data)\n",
    "\n",
    "term_by_document_matirx: sparse.csr_matrix = cache.load('term_by_document_sparse_matrix.dump')\n",
    "if term_by_document_matirx is None:\n",
    "    print('creating sparse matrix')\n",
    "    term_by_document_matirx = create_sparse(articles_data, sizeof_total, idf)\n",
    "print(\"term by document matrix size: {}x{}\".format(term_by_document_matirx.shape[0],\n",
    "                                                   term_by_document_matirx.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> caching articles_data.dump\n",
      "> caching wordset.dump\n",
      "> caching term_by_document_sparse_matrix.dump\n",
      "> caching total_bag_of_words.dump\n",
      "> caching idf.dump\n"
     ]
    }
   ],
   "source": [
    "cache.save('articles_data.dump', articles_data)\n",
    "cache.save('wordset.dump', wordset)\n",
    "cache.save('term_by_document_sparse_matrix.dump', term_by_document_matirx)\n",
    "cache.save('total_bag_of_words.dump', total_bag_of_words)\n",
    "cache.save('idf.dump', idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Program pozwalający na wyszukiwanie artykułów\n",
    "Dla czytelności wyłączono wypisywanie całej treści artykułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query, word_list):\n",
    "    query = query.lower()\n",
    "    words_dict = {word: index for index, word in enumerate(word_list)}\n",
    "    words = re.findall(r'\\w+', query)\n",
    "\n",
    "    vec_query = np.zeros(len(word_list), dtype=int)\n",
    "    for w in words:\n",
    "        if w in words_dict.keys():\n",
    "            vec_query[words_dict[w]] += 1\n",
    "\n",
    "    if not np.any(vec_query):\n",
    "        print(\"No results\")\n",
    "        return\n",
    "\n",
    "    return vec_query\n",
    "\n",
    "\n",
    "def print_search_results(res, k, query):\n",
    "    res.sort(key=operator.itemgetter(0), reverse=True)\n",
    "    print(\"Found articles for query [{}]:\".format(query))\n",
    "    for res_entry in res[:k]:\n",
    "        print('> ' + res_entry[1].title.replace(\"_\", \" \"))\n",
    "\n",
    "#     print(\"\\n\\nFull articles:\")\n",
    "#     for res_entry in res[:k]:\n",
    "#         print(res_entry[1].print_contents())\n",
    "#         print('\\n')\n",
    "#         print('-' * 40)\n",
    "\n",
    "\n",
    "def do_query(query, k, word_list, articles):\n",
    "    vec_query = parse_query(query, word_list)\n",
    "\n",
    "    q_norm = np.linalg.norm(vec_query)\n",
    "    vec_query = vec_query.T\n",
    "    res = []\n",
    "    for a in articles:\n",
    "        divider = q_norm * a.words_vec_norm\n",
    "        prod = vec_query @ a.words_vec\n",
    "        cos_theta = prod / divider\n",
    "        res.append((cos_theta, a))\n",
    "\n",
    "    print_search_results(res, k, query)\n",
    "        \n",
    "        \n",
    "        \n",
    "# reassign variables, just for readibility\n",
    "# articles - list with all of documents (words vectors + bag of words)\n",
    "# word_list - bag_of_words_dict.keys()\n",
    "# A - sparse matrix, columns are words vectors from articles_data\n",
    "articles, word_list, A = articles_data, wordset, term_by_document_matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe wyszukania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Action film]:\n",
      "> The Blue Mansion\n",
      "> Doni Sagali\n",
      "> Ostend Film Festival\n",
      "> Alankrita Shrivastava\n",
      "> LA Rebellion\n"
     ]
    }
   ],
   "source": [
    "do_query(\"Action film\", 5, word_list, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Winston Churchill]:\n",
      "> Lancelot Royle\n",
      "> No 47 Royal Marine Commando\n",
      "> Johnny Murtagh\n",
      "> 12th Yorkshire Parachute Battalion\n",
      "> 1828 United States presidential election in Ohio\n"
     ]
    }
   ],
   "source": [
    "do_query(\"Winston Churchill\", 5, word_list, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Beautiful places on earth]:\n",
      "> Tolkiens legendarium\n",
      "> Fossil Fighters\n",
      "> Nanorchestidae\n",
      "> 7 Persei\n",
      "> Ek Khiladi Ek Haseena TV series\n"
     ]
    }
   ],
   "source": [
    "do_query(\"Beautiful places on earth\", 5, word_list, articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktywna wyszukiwarka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946cb644edfe476b823b9f007eb01d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Results num'), IntSlider(value=10, max=30, min=1)), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Text, Label, IntSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def btn(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        how_many = form.children[0].children[1].value\n",
    "        text_to_search = form.children[1].children[1].value\n",
    "        if len(text_to_search) > 1:\n",
    "            do_query(text_to_search, how_many, word_list, articles)\n",
    "        else:\n",
    "            output.append_stdout(\"Text is too short\")\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    display='flex',\n",
    "    flex_flow='row',\n",
    "    justify_content='space-between'\n",
    ")\n",
    "\n",
    "form_items = [\n",
    "    Box([Label(value='Results num'), IntSlider(min=1, max=30, value=10, descritpion='k_')], layout=form_item_layout),\n",
    "    Box([Label(value='Query'), Text(placeholder=\"Wpisz zapytanie\", descritpion='query_')], layout=form_item_layout),\n",
    "    Box([Label(), Button(description=\"Search!\")], layout=form_item_layout)\n",
    "]\n",
    "\n",
    "form = Box(form_items, layout=Layout(\n",
    "    display='flex',\n",
    "    flex_flow='column',\n",
    "    align_items='stretch',\n",
    "    width='50%'\n",
    "))\n",
    "output = Output()\n",
    "form.children[2].children[1].on_click(btn)\n",
    "form # you may need to run this cell manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af0c3f338e4ce5ac5b4f00a35bfb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output) # place for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Normalizacja wektorów\n",
    "Znormalizowano wektory przechowywane w klasie ArticleData oraz zbudowano na nowo macierz rzadką A używając nowych wektorów. Wektor zapytania także został znormalizowany. Wykonano takie same wyszukiwania jak w wariancie bez normalizacji w celu weryfikacji poprawności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating new sparse matrix with new vectors\n",
      "> caching A_normalized.dump\n"
     ]
    }
   ],
   "source": [
    "def normalize_vectors(articles):\n",
    "    for a in articles:\n",
    "        a.normalize_word_vec()\n",
    "        \n",
    "\n",
    "normalize_vectors(articles)\n",
    "A_normalized: sparse.csr_matrix = cache.load('A_normalized.dump')\n",
    "if A_normalized is None:\n",
    "    print('calculating new sparse matrix with new vectors')\n",
    "    A_normalized = create_sparse(articles, len(word_list), idf)\n",
    "    cache.save('A_normalized.dump', A_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query2(query, k, word_list, articles, A):\n",
    "    vec_query = parse_query(query, word_list)\n",
    "    vec_query = vec_query / np.linalg.norm(vec_query)\n",
    "    \n",
    "    res = vec_query.T @ A\n",
    "    probabilities = []\n",
    "    for i, cos_theta in enumerate(res):\n",
    "        probabilities.append((cos_theta, articles[i]))\n",
    "\n",
    "    print_search_results(probabilities, k, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Action film]:\n",
      "> The Blue Mansion\n",
      "> Doni Sagali\n",
      "> Ostend Film Festival\n",
      "> Alankrita Shrivastava\n",
      "> LA Rebellion\n"
     ]
    }
   ],
   "source": [
    "do_query2(\"Action film\", 5, word_list, articles, A_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Winston Churchill]:\n",
      "> Lancelot Royle\n",
      "> No 47 Royal Marine Commando\n",
      "> Johnny Murtagh\n",
      "> 12th Yorkshire Parachute Battalion\n",
      "> 1828 United States presidential election in Ohio\n"
     ]
    }
   ],
   "source": [
    "do_query2(\"Winston Churchill\", 5, word_list, articles, A_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Beautiful places on earth]:\n",
      "> Tolkiens legendarium\n",
      "> Fossil Fighters\n",
      "> Nanorchestidae\n",
      "> 7 Persei\n",
      "> Ek Khiladi Ek Haseena TV series\n"
     ]
    }
   ],
   "source": [
    "do_query2(\"Beautiful places on earth\", 5, word_list, articles, A_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Normalizacja wektorów\n",
    "Zastosowanie SVD, low rank approximation oraz nowej miary prawdopodobieństwa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSVD(A, rank):\n",
    "    U, S, VT = sparse.linalg.svds(A, rank)\n",
    "    return U @ np.diag(S) @ VT\n",
    "\n",
    "\n",
    "def do_query3(query, k, word_list, articles, A, rank):\n",
    "    vec_query = parse_query(query, word_list)\n",
    "    q_norm = np.linalg.norm(vec_query)\n",
    "\n",
    "    A_k = getSVD(A, rank)\n",
    "\n",
    "    res = []\n",
    "    for i, ak_row in enumerate(A_k.T):\n",
    "        prod = vec_query.T @ ak_row\n",
    "        cos_fi = prod / (q_norm * np.linalg.norm(ak_row))\n",
    "        res.append((cos_fi, articles[i]))\n",
    "\n",
    "    print_search_results(res, k, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Action film]:\n",
      "> Ömer Faruk Sorak\n",
      "> The Murderer Dimitri Karamazov\n",
      "> Lakshmipati\n",
      "> Aashiqui 2015 film\n",
      "> Doni Sagali\n"
     ]
    }
   ],
   "source": [
    "rank = 120\n",
    "do_query3(\"Action film\", 5, word_list, articles, A, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Winston Churchill]:\n",
      "> No 47 Royal Marine Commando\n",
      "> Pseudoalteromonas denitrificans\n",
      "> Johnny Murtagh\n",
      "> List of Royal Air Force operations\n",
      "> K1 World Grand Prix 2000 Final\n"
     ]
    }
   ],
   "source": [
    "do_query3(\"Winston Churchill\", 5, word_list, articles, A, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles for query [Beautiful places on earth]:\n",
      "> Tolkiens legendarium\n",
      "> Fossil Fighters\n",
      "> Elves Hill\n",
      "> Crassispira martiae\n",
      "> Zeder\n"
     ]
    }
   ],
   "source": [
    "do_query3(\"Beautiful places on earth\", 5, word_list, articles, A, rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Porównanie wyników z odszumianiem i bez. Wpływ IDF na wyniki.\n",
    "Jak widać wyniki wyszukiwań w trzech przypadkach są bardzo zbliżone do siebie. Doświadczalno wybrano k=120 jako najmniejszą i nalepszą wartość, dla której wyniki są akceptowalne. Wyniki z odszumianiem są bardziej trafne niż bez odszumiania, jednak koszt obliczania SVD dla dużych macierzy może być odczuwalny. Przekształcenie IDF pozwala na redukcję znaczenia słów, które występują wielokrotnie w dokumentach, przez co wyniki są bardziej konkretne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Źródła:\n",
    "* [Latent semantic indexing](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)\n",
    "* [Latent Semantic Analysis](https://www.engr.uvic.ca/~seng474/svd.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
