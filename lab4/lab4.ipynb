{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody Obliczeniowe w Nauce i Technice\n",
    "## Laboratorium 4 - Singular Value Decomposition (Wyszukiwarka)\n",
    "### Albert Gierlach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Przygotowanie danych\n",
    "Dane przygotowano za pomocą wiki-crawlera. Wykorzystano skrypt w Pythonie (https://github.com/bornabesic/wikipedia-crawler), dostosowując go do potrzeb zadania (dodanie opcji, która pozwala pobrać N artykułów). Źródła (wikipedia.py oraz crawler.py) są dostępne w archiwum z zadaniem.\n",
    "\n",
    "Użycie:\n",
    "```\n",
    "python crawler.py N subdomain\n",
    "```\n",
    "gdzie N to liczba dokumentow do pobrania, a 'subdomain' to subdomena (użyto wartości 'en').\n",
    "Dla polepszenia rezultatów zapewniono, że długość artykułu będzie większa niż 1000 znaków.\n",
    "\n",
    "Dane w formacie .txt pobierane są do folderu ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2., 3. Określenie bag-of-words \n",
    "Stworzono klasę, która będzie przechowywać dane jednego dokumentu oraz odpowiednie jej metody, które będą wykorzystane później. Odrzucono kilka słów, które powinny zostać zignorowane podczas wyszukiwania artykułów. Stworzono także klasę, która będzie odpowiadać za cache'owanie wyliczonych wektorów i macierzy, gdyż operacja ta trwa dość długo. Zastosowanie takiej klasy pozwala na jednokrotne wyliczenie wartości, a później wystarczy wczytać gotowe dane. Pierwsze uruchomienie trwa max 5 minut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Any\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    cache_dir = \"./cache\"  # place for storing calculated matrices, etc\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loaded = set()\n",
    "\n",
    "        if not os.path.exists(CacheManager.cache_dir):\n",
    "            os.makedirs(CacheManager.cache_dir)\n",
    "\n",
    "    def was_loaded(self, filename):\n",
    "        return filename in self.loaded\n",
    "\n",
    "    def save(self, filename, object):\n",
    "        if self.was_loaded(filename):\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open('{}/{}'.format(CacheManager.cache_dir, filename), \"wb\") as f:\n",
    "                pickle.dump(object, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"> caching \" + filename)\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    def load(self, filename):\n",
    "        try:\n",
    "            with open('{}/{}'.format(CacheManager.cache_dir, filename), \"rb\") as f:\n",
    "                res = pickle.load(f)\n",
    "                print(\"> using cached \" + filename)\n",
    "                self.loaded.add(filename)\n",
    "                return res\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "class ArticleData:\n",
    "    ignored_words = [\"a\", \"the\", \"of\", \"is\"]  # and probably more\n",
    "\n",
    "    def __init__(self, title):\n",
    "        self.title = title.split('.')[0]\n",
    "        self.bag_of_words = Counter()\n",
    "        self.words_vec = None\n",
    "        self.words_vec_norm = None\n",
    "\n",
    "    def load_bag_of_words(self, path):\n",
    "        with open(path, \"rt\", encoding='utf-8') as f:\n",
    "            words = re.findall(r'\\w+', f.read().lower())\n",
    "            loaded_words = [word for word in words if len(word) > 2]\n",
    "            self.bag_of_words.update(loaded_words)\n",
    "\n",
    "        for ignore_token in ArticleData.ignored_words:\n",
    "            del self.bag_of_words[ignore_token]\n",
    "\n",
    "    def create_full_bag_of_words(self, keyset, size):\n",
    "        self.words_vec = np.zeros(size)  # d_j\n",
    "        for i, k in enumerate(keyset):\n",
    "            self.words_vec[i] = self.bag_of_words[k]\n",
    "\n",
    "        self.words_vec_norm = np.linalg.norm(self.words_vec)\n",
    "\n",
    "    def print_contents(self):\n",
    "        with open('{}/{}.txt'.format(data_dir, self.title), \"rt\", encoding='utf-8') as f:\n",
    "            print(f.read())\n",
    "\n",
    "    def normalize_word_vec(self):\n",
    "        self.words_vec = self.words_vec / np.linalg.norm(self.words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> using cached articles_data.dump\n",
      "total number of articles 1500\n",
      "> using cached total_bag_of_words.dump\n",
      "> using cached wordset.dump\n",
      "total number of words: 68581\n",
      "creating bag of words for every article\n",
      "created 1500 bags, every has 68581 elements\n"
     ]
    }
   ],
   "source": [
    "cache = CacheManager()\n",
    "\n",
    "articles_data: List[ArticleData] = cache.load('articles_data.dump')\n",
    "if articles_data is None:\n",
    "    articles_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        a_data = ArticleData(file)\n",
    "        a_data.load_bag_of_words(\"{}/{}\".format(data_dir, file))\n",
    "        articles_data.append(a_data)\n",
    "print(\"total number of articles {}\".format(len(articles_data)))\n",
    "\n",
    "total_bag_of_words: Counter = cache.load('total_bag_of_words.dump')\n",
    "if total_bag_of_words is None:\n",
    "    total_bag_of_words = Counter()\n",
    "    for article in articles_data:\n",
    "        total_bag_of_words += article.bag_of_words\n",
    "\n",
    "sizeof_total = len(total_bag_of_words)\n",
    "wordset: List[Any] = cache.load('wordset.dump')\n",
    "if wordset is None:\n",
    "    wordset = list(total_bag_of_words.keys())\n",
    "print(\"total number of words: {}\".format(sizeof_total))\n",
    "\n",
    "print(\"creating bag of words for every article\")\n",
    "if not cache.was_loaded('articles_data.dump'):\n",
    "    for article in articles_data:\n",
    "        article.create_full_bag_of_words(wordset, sizeof_total)\n",
    "print(\"created {} bags, every has {} elements\".format(len(articles_data), sizeof_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4., 5.  Rzadka macierz wektorów cech oraz IDF\n",
    "Do budowy rzadkiej macierzy wykorzystano funckję crs_matrix(). Czas operacji 3-5 minut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDF(wordset, articles_data):\n",
    "    articles_num = len(articles_data)\n",
    "    idf = []\n",
    "    for word in wordset:\n",
    "        cnt = 0\n",
    "        for article in articles_data:\n",
    "            if article.bag_of_words[word] != 0:\n",
    "                cnt += 1\n",
    "\n",
    "        idf.append(np.log10(articles_num/cnt))\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def create_sparse(articles_data, sizeof_total, idf):\n",
    "    row = []\n",
    "    column = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(articles_data)):\n",
    "        article = articles_data[i]\n",
    "        for j in range(sizeof_total):\n",
    "            if article.words_vec[j] != 0:\n",
    "                row.append(j)\n",
    "                column.append(i)\n",
    "                data.append(article.words_vec[j] * idf[j])\n",
    "\n",
    "\n",
    "    term_by_document_matirx = sparse.csr_matrix((data, (row, column)), shape=(sizeof_total, len(articles_data)))\n",
    "    return term_by_document_matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating idf\n",
      "> using cached idf.dump\n",
      "creating sparse matrix\n",
      "> using cached term_by_document_sparse_matrix.dump\n",
      "term by document matrix size: 68581x1500\n"
     ]
    }
   ],
   "source": [
    "print('calculating idf')\n",
    "idf: List[Any] = cache.load('idf.dump')\n",
    "if idf is None:\n",
    "    idf = getIDF(wordset, articles_data)\n",
    "\n",
    "print('creating sparse matrix')\n",
    "term_by_document_matirx: sparse.csr_matrix = cache.load('term_by_document_sparse_matrix.dump')\n",
    "if term_by_document_matirx is None:\n",
    "    term_by_document_matirx = create_sparse(articles_data, sizeof_total, idf)\n",
    "print(\"term by document matrix size: {}x{}\".format(term_by_document_matirx.shape[0],\n",
    "                                                   term_by_document_matirx.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save('articles_data.dump', articles_data)\n",
    "cache.save('wordset.dump', wordset)\n",
    "cache.save('term_by_document_sparse_matrix.dump', term_by_document_matirx)\n",
    "cache.save('total_bag_of_words.dump', total_bag_of_words)\n",
    "cache.save('idf.dump', idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Program pozwalający na wyszukiwanie artykułów\n",
    "Stworzono nowy plik lab4_search_engine.py, który będzie odpowiedzialny za przetwarzanie danych. Na początku wczytamy zcache'owane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_query(query, k, word_list, articles):\n",
    "    query = query.lower()\n",
    "    words_dict = {word: index for index, word in enumerate(word_list)}\n",
    "    words = re.findall(r'\\w+', query)\n",
    "\n",
    "    vec_query = np.zeros(len(word_list), dtype=int)\n",
    "    for w in words:\n",
    "        if w in words_dict.keys():\n",
    "            vec_query[words_dict[w]] += 1\n",
    "\n",
    "    if not np.any(vec_query):\n",
    "        print(\"No results\")\n",
    "        return\n",
    "\n",
    "    q_norm = np.linalg.norm(vec_query)\n",
    "    vec_query = vec_query.T\n",
    "    res = []\n",
    "    for a in articles:\n",
    "        divider = q_norm * a.words_vec_norm\n",
    "        prod = vec_query @ a.words_vec\n",
    "        cos_theta = prod/divider\n",
    "        res.append((cos_theta, a))\n",
    "\n",
    "    res.sort(key=operator.itemgetter(0), reverse=True)\n",
    "    print(\"Found articles:\")\n",
    "    for res_entry in res[:k]:\n",
    "        print('> ' + res_entry[1].title)\n",
    "\n",
    "    print(\"\\n\\nFull articles:\")\n",
    "    for res_entry in res[:k]:\n",
    "        print(res_entry[1].print_contents())\n",
    "        print('\\n')\n",
    "        print('*' * 40)\n",
    "        \n",
    "        \n",
    "        \n",
    "# reassign variables, just for readibility\n",
    "# articles - list with all of documents (words vectors + bag of words)\n",
    "# word_list - bag_of_words_dict.keys()\n",
    "# A - sparse matrix, columns are words vectors from articles_data\n",
    "articles, word_list, A = articles_data, wordset, term_by_document_matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe wyszukania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_query(\"Action film\", 3, word_list, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_query(\"Winston Churchill\", 3, word_list, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_query(\"Poland\", 3, word_list, articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktywna wyszukiwarka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Text, Label, IntSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def btn(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        how_many = form.children[0].children[1].value\n",
    "        text_to_search = form.children[1].children[1].value\n",
    "        if len(text_to_search) > 1:\n",
    "            do_query(text_to_search, how_many, word_list, articles)\n",
    "        else:\n",
    "            output.append_stdout(\"Text is too short\")\n",
    "\n",
    "form_item_layout = Layout(\n",
    "    display='flex',\n",
    "    flex_flow='row',\n",
    "    justify_content='space-between'\n",
    ")\n",
    "\n",
    "form_items = [\n",
    "    Box([Label(value='Results num'), IntSlider(min=1, max=30, value=10, descritpion='k_')], layout=form_item_layout),\n",
    "    Box([Label(value='Query'), Text(placeholder=\"Wpisz zapytanie\", descritpion='query_')], layout=form_item_layout),\n",
    "    Box([Label(), Button(description=\"Search!\")], layout=form_item_layout)\n",
    "]\n",
    "\n",
    "form = Box(form_items, layout=Layout(\n",
    "    display='flex',\n",
    "    flex_flow='column',\n",
    "    align_items='stretch',\n",
    "    width='50%'\n",
    "))\n",
    "output = Output()\n",
    "form.children[2].children[1].on_click(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76acd0ad5d24357b5cfcf26293a474f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Results num'), IntSlider(value=10, max=30, min=1)), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac420cf278634b748a265a8ed10a295c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output) # place for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Normalizacja wektorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vectors(articles):\n",
    "    for a in articles:\n",
    "        a.normalize_word_vec()\n",
    "        \n",
    "normalize_vectors(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
